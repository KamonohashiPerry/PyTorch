{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamonohashiPerry/PyTorch/blob/master/PyTorch_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psUl7qkT4uNE",
        "colab_type": "text"
      },
      "source": [
        "# PyTorchのインストール\n",
        "+ Colabの編集→ノートブックの設定でGPUモードにすることをお忘れなく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk5T777J4OE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMAYS8QJ4wHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpbBwXiN40MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPWZNZX346jj",
        "colab_type": "text"
      },
      "source": [
        "インストールできたかの確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN__77Ap417w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPctnygX4-12",
        "colab_type": "code",
        "outputId": "2728a046-23dc-4ea4-c448-4dd246c801a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.tensor([1, 2, 3]).to(\"cuda:0\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iiN-QLZ5BiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vFtGVjP5JKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # drive mean root directory of  google drive\n",
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive\n",
        "# !ls drive/\"Colab Notebooks\"/PyTorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PSPHCNt5JWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import (Dataset, DataLoader, TensorDataset)\n",
        "import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iELizoi95Jbq",
        "colab_type": "code",
        "outputId": "87d19597-57de-43ed-94f2-9da4954d01ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-17 11:57:10--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4752884 (4.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.53M  2.91MB/s    in 1.6s    \n",
            "\n",
            "2019-12-17 11:57:17 (2.91 MB/s) - ‘spa-eng.zip’ saved [4752884/4752884]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SqR5xdm5gyl",
        "colab_type": "code",
        "outputId": "7755361c-ad12-49bd-88ba-4cdf1e6f0052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!unzip spa-eng.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI5clEZu5sGe",
        "colab_type": "code",
        "outputId": "f92842d7-ce26-4786-f400-456483c62572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt  \u001b[0m\u001b[01;34msample_data\u001b[0m/  spa-eng.zip  spa.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT2IPp3m11gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"spa.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adJhKR1w5te9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import collections\n",
        "import itertools\n",
        "\n",
        "remove_marks_regex = re.compile(\"[\\,\\(\\)\\[\\]\\*:;¿¡]|<.*?>\")\n",
        "shift_marks_regex = re.compile(\"([?!\\.])\")\n",
        "\n",
        "unk = 0\n",
        "sos = 1\n",
        "eos = 2\n",
        "\n",
        "def normalize(text):\n",
        "  # 小文字にする\n",
        "  text = text.lower()\n",
        "  # 不要な文字を除去\n",
        "  text = remove_marks_regex.sub(\"\", text)\n",
        "  # ?!.と単語の間に空白を挿入\n",
        "  text = shift_marks_regex.sub(r\" \\1\", text)\n",
        "  return text\n",
        "\n",
        "def parse_line(line):\n",
        "  line = normalize(line.strip())\n",
        "  # 翻訳元(src)と翻訳先(trg)それぞれのトークンのリストを作る\n",
        "  src, trg, _ = line.split(\"\\t\")\n",
        "  src_tokens = src.strip().split()\n",
        "  trg_tokens = trg.strip().split()\n",
        "  return src_tokens, trg_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U12yoztS7VBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(tokens):\n",
        "  # ファイル中の全ての文章でのトークンの出現数を数える\n",
        "  counts = collections.Counter(tokens)\n",
        "  # トークンの出現数の多い順に並べる\n",
        "  sorted_counts = sorted(counts.items(),\n",
        "                            key=lambda c: c[1], reverse=True)\n",
        "  # 3つのタグを追加して正引きリストと逆引き用辞書を作る\n",
        "  word_list = [\"<UNK>\", \"<SOS>\", \"<EOS>\"] + [x[0] for x in sorted_counts]\n",
        "  word_dict = dict((w, i) for i, w in enumerate(word_list))\n",
        "  return word_list, word_dict\n",
        "\n",
        "def words2tensor(words, word_dict, max_len, padding=0):\n",
        "  # 末尾に終了タグをつける\n",
        "  words = words + [\"<EOS>\"]\n",
        "  # 辞書を利用して数値のリストに変換する\n",
        "  words = [word_dict.get(w, 0) for w in words]\n",
        "  seq_len = len(words)\n",
        "  # 長さがmax_len以下の場合はパディングする\n",
        "  if seq_len < max_len + 1:\n",
        "    words = words + [padding] * (max_len + 1 - seq_len)\n",
        "    # Tensorに変換して返す\n",
        "  return torch.tensor(words, dtype=torch.int64), seq_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h9fd9QCA6SLZ",
        "colab": {}
      },
      "source": [
        "class TranslationPairDataset(Dataset):\n",
        "  def __init__(self, path, max_len=15):\n",
        "    # 単語数が多い文章をフィルタリングする関数\n",
        "    def filter_pair(p):\n",
        "      return not (len(p[0]) > max_len or len(p[1]) > max_len )\n",
        "\n",
        "    # ファイルを開き、パース/フィルタリングをする\n",
        "    with open(path) as fp:\n",
        "      pairs = map(parse_line, fp)\n",
        "      pairs = filter(filter_pair, pairs)\n",
        "      pairs = list(pairs)\n",
        "\n",
        "    # 文章のペアをソースとターゲットに分ける\n",
        "    src = [p[0] for p in pairs]\n",
        "    trg = [p[1] for p in pairs]\n",
        "    # それぞれの語彙集を作成する\n",
        "    self.src_word_list, self.src_word_dict = build_vocab(itertools.chain.from_iterable(src))\n",
        "    self.trg_word_list, self.trg_word_dict = build_vocab(itertools.chain.from_iterable(trg))\n",
        "\n",
        "    # 語彙集を使用してTensorに変換する\n",
        "    self.src_data = [words2tensor(\n",
        "        words, self.src_word_dict, max_len)\n",
        "          for words in src]\n",
        "\n",
        "    # -100でパディングすることでPyTorchの損失関数の計算に含めないようにするらしい。\n",
        "    # 可変長の系列の扱いが容易になるらしい。\n",
        "    self.trg_data = [words2tensor(\n",
        "        words, self.trg_word_dict, max_len, -100)\n",
        "          for words in trg]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src, lsrc = self.src_data[idx]\n",
        "    trg, ltrg = self.trg_data[idx]\n",
        "    return src, lsrc, trg, ltrg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5079f3ca-5839-4757-a921-1b09bb2994c2",
        "id": "oP_S9ajj6Pm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PCA1S0yx1TI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DatasetとDataLoaderの作成\n",
        "batch_size = 64\n",
        "max_len = 10\n",
        "\n",
        "path = \"/content/spa.txt\"\n",
        "ds = TranslationPairDataset(path, max_len=max_len)\n",
        "loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m9FlKWE5D6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoderの作成\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_embeddings,\n",
        "               embedding_dim=50,\n",
        "               hidden_size=50,\n",
        "               num_layers=1,\n",
        "               dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim,\n",
        "                            padding_idx=0)\n",
        "    self.lstm = nn.LSTM(embedding_dim,\n",
        "                        hidden_size,\n",
        "                        num_layers,\n",
        "                        batch_first=True,\n",
        "                        dropout=dropout)\n",
        "    \n",
        "    def forward(self, x, h0=None, l=None):\n",
        "      x = self.emb(x)\n",
        "      if l is not None:\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, l, batch_first=True)\n",
        "      # 内部状態のみをDecoderにわたすので、出力は破棄している。\n",
        "      _, h = self.lstm(x, h0)\n",
        "      return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg5SdijN9Fu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoderの作成\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_embeddings,\n",
        "               embedding_dim=50,\n",
        "               hidden_size=50,\n",
        "               num_layers=1,\n",
        "               dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(num_embeddings, embedding_dim,padding_idx=0)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_size,\n",
        "                        num_layers,batch_first=True,\n",
        "                        dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_size, num_embeddings)\n",
        "\n",
        "  def forward(self, x, h, l=None):\n",
        "    x = self.emb(x)\n",
        "    if l is not None:\n",
        "      x = nn.utils.rnn.pack_padded_sequence(x, l, batch_first=True)\n",
        "      x, h = self.lstm(x, h)\n",
        "      if l is not None:\n",
        "        x = nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0)[0]\n",
        "      x = self.linear(x)\n",
        "      return x, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaK3zAWU-z4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}